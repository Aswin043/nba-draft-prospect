{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caa4b92",
   "metadata": {},
   "source": [
    "# From Draft to Stardom: Predicting NBA Success from College and Combine Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8feec",
   "metadata": {},
   "source": [
    "### Goal: Predict whether a drafted NBA player becomes an All-Star, starter, or bench player using pre-draft data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NBA endpoints needed \n",
    "#--> /DraftHistory\n",
    "#--> /DraftCombineStats\n",
    "#i also need raw player stats from college games which will be web scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf437977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.basketball-reference.com/draft/NBA_2025.html'  \n",
    "\n",
    "# Get HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the draft table\n",
    "table = soup.find('table', {'id': 'stats'})\n",
    "\n",
    "# Extract column headers\n",
    "headers = [th.getText() for th in table.find('thead').findAll('th')][1:]  # skip rank header\n",
    "\n",
    "# Extract all rows\n",
    "rows = table.find('tbody').findAll('tr')\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    if 'class' in row.attrs and 'thead' in row['class']:\n",
    "        continue  # Skip header rows within body\n",
    "\n",
    "    cells = row.find_all('td')\n",
    "    row_data = []\n",
    "    player_link = None\n",
    "\n",
    "    for cell in cells:\n",
    "        if cell.get('data-stat') == 'player':\n",
    "            a_tag = cell.find('a')\n",
    "            if a_tag:\n",
    "                player_link = 'https://www.basketball-reference.com' + a_tag['href']\n",
    "                row_data.append(cell.get_text(strip=True))\n",
    "            else:\n",
    "                row_data.append(None)\n",
    "        else:\n",
    "            row_data.append(cell.get_text(strip=True) if cell else None)\n",
    "\n",
    "    if len(row_data) < len(headers):\n",
    "        # Add Nones for missing columns\n",
    "        row_data += [None] * (len(headers) - len(row_data))\n",
    "\n",
    "    row_data.append(player_link)\n",
    "    data.append(row_data)\n",
    "\n",
    "# Add 'Player_Link' column\n",
    "headers.append('Player_Link')\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('nba_draft_2025.csv', index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Load your cleaned draft table (with player links)\n",
    "df = pd.read_csv(\"nba_draft_25.csv\")\n",
    "\n",
    "# Keep only desired columns\n",
    "keep_cols = ['Pk', 'Tm', 'Player', 'College', 'Player_Link']\n",
    "df = df[keep_cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f06ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "# List of user agents to rotate through\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; rv:114.0) Gecko/20100101 Firefox/114.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko)\"\n",
    "]\n",
    "\n",
    "def get_college_career_stats(player_url):\n",
    "    try:\n",
    "        # Random delay between 15 and 20 seconds\n",
    "        delay = random.uniform(15, 20)\n",
    "        print(f\"Sleeping for {delay:.2f} seconds...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Pick a random user-agent\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(user_agents)\n",
    "        }\n",
    "\n",
    "        res = requests.get(player_url, headers=headers)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        # Try direct table first (for newer players)\n",
    "        table = soup.find('table', {'id': 'all_college_stats'})\n",
    "\n",
    "        # Fallback to parsing from HTML comments (for older pages)\n",
    "        if table is None:\n",
    "            comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "            for comment in comments:\n",
    "                if 'all_college_stats' in comment:\n",
    "                    comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "                    table = comment_soup.find('table', {'id': 'all_college_stats'})\n",
    "                    break\n",
    "\n",
    "        if table is None:\n",
    "            print(f\"[X] No college stats table found for {player_url}\")\n",
    "            return {}\n",
    "\n",
    "        # Locate 'Career' row\n",
    "        career_row = None\n",
    "        for row in table.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th and th.text.strip().lower() == \"career\":\n",
    "                career_row = row\n",
    "                break\n",
    "\n",
    "        if not career_row:\n",
    "            print(f\"[X] No 'Career' row found in table for {player_url}\")\n",
    "            return {}\n",
    "\n",
    "        # Extract stats\n",
    "        desired_stats = [\"fg_pct\", \"fg3_pct\", \"ft_pct\", \"mp_per_g\", \"pts_per_g\", \"trb_per_g\", \"ast_per_g\"]\n",
    "        stats = {stat: None for stat in desired_stats}\n",
    "\n",
    "        for td in career_row.find_all('td'):\n",
    "            stat_name = td.get('data-stat')\n",
    "            if stat_name in desired_stats:\n",
    "                stats[stat_name] = td.text.strip()\n",
    "\n",
    "        return stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed for {player_url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track stats\n",
    "college_stats = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    player_url = row[\"Player_Link\"]\n",
    "    print(f\"Scraping: {row['Player']} -> {player_url}\")\n",
    "    stats = get_college_career_stats(player_url)\n",
    "    print(f\"Stats for {row['Player']}: {stats}\")\n",
    "    college_stats.append(stats)\n",
    "    time.sleep(1)  # to avoid rate-limiting\n",
    "\n",
    "# Convert scraped stats into a DataFrame\n",
    "stats_df = pd.DataFrame(college_stats)\n",
    "final_df = pd.concat([df.reset_index(drop=True), stats_df.reset_index(drop=True)], axis=1)\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf128bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to CSV\n",
    "final_df.to_csv('nba_draft_2025_with_college_statsv1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ec5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "nba_df= pd.read_csv('nba_draft_2025_with_college_statsv1.csv')\n",
    "nba_df=nba_df.drop(columns=\"Player_Link\")\n",
    "nba_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f48a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_cols = [\"fg_pct\", \"fg3_pct\", \"ft_pct\", \"mp_per_g\", \"pts_per_g\", \"trb_per_g\", \"ast_per_g\"]\n",
    "nba_df = nba_df.dropna(subset=stat_cols, how='all')\n",
    "nba_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f00221",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df[\"College\"] = nba_df[\"College\"].fillna(\"International\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a98bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.to_csv('nba_draft_2025_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
